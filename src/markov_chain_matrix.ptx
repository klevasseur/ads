<?xml version="1.0" encoding="UTF-8" ?>
<subsection xml:id="ss-markov_chain_matrix"><title>Matrix Analysis of Markov Chains</title>

<p>In <xref ref="ex-markov_chain" text="title" /> 
we introduced the idea of a Markov Chain. The movement in such a process can be visualized with a transition graph,   Computation of probabilities related to such a process is often done using matrix algebra.  The basic component in the computations is the transition matrix for the process.</p>

<definition xml:id="def-transition-matrix">
	<title>Transition Matrix</title>
	<idx>Transition Matrix</idx>
	<statement>
		<p>Given a Markov Chain with states <m>s_1, s_2, \dots, s_n</m>, the transition matrix for the process is the <m>n \times n</m> matrix <m>T</m> where <m>T_{i,j}</m> is the probability that if the process is in state <m>s_i</m>, its next state is <m>s_j</m>.</p>
	</statement>
</definition>

<p>
The ordering of states can be arbitrary, but is most convenient to list the absorbins states of the process, the ones that are permanently entered, last. The transition matrix for the tennis game process with states ordered  Ad-In, Deuce, Ad-Out, Hold, Break is</p>
<p>	<me>
			T=\left(
			\begin{array}{ccccc}
			 0 &amp; 0.4 &amp; 0 &amp; 0.6 &amp; 0 \\
			 0.6 &amp; 0 &amp; 0.4 &amp; 0 &amp; 0 \\
			 0 &amp; 0.6 &amp; 0 &amp; 0 &amp; 0.4 \\
			 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
			 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
			\end{array}
			\right).
	</me></p>
<p>	Representing a Markov chain by its transition matrix makes it easy to compute probabilities associated with it.  For example, we might want to know the probability that if the game is in Deuce that after two points have been played it is back in Deuce.  We can reason that to return to Deuce the process can go from Deuce to Ad-In and back to Deuce.  This happens with probability
		</p>
<p>
	<me>T_{2,1}\cdot T_{1,2} =0.6 \cdot 0.4 = 0.24.</me>
</p>

<p>The other way to return to Deuce is to go to Ad-Out and then back to Deuce. This happens with probability
	<me>T_{2,3}\cdot T_{3,2} =0.4 \cdot 0.6 = 0.24.</me>
Since these paths in the graph are mutually exclusive (they can't both happen), the probability of being in Deuce after two points is the sum 0.48.  The two products we added here are the nonzero terms of the innner product of the second row of <m>T</m> with the second column of <m>T</m>.  If we compute the <m>T^2</m>, we get this result in the second row, second column.  But the whole matrix gives us the transition probabilities between any pair of states after two points have been played.  Here is the whole matrix:
	<me>
			T^2=\left(
				\begin{array}{ccccc}
				 0.24 &amp; 0. &amp; 0.16 &amp; 0.6 &amp; 0. \\
				 0. &amp; 0.48 &amp; 0. &amp; 0.36 &amp; 0.16 \\
				 0.36 &amp; 0. &amp; 0.24 &amp; 0. &amp; 0.4 \\
				 0. &amp; 0. &amp; 0. &amp; 1. &amp; 0. \\
				 0. &amp; 0. &amp; 0. &amp; 0. &amp; 1. \\
				\end{array}
				\right).
	</me>
</p>
<p>
In general, the <m>n^{th}</m> power of <m>T</m> will give us the probabilities after <m>n</m> points have been played. For example, starting at Deuce the probabiity that the server will have held serve is <m>0.674667</m> while a service break would have taken place with probability <m>0.299852</m>,  The likelihood that the game would still be undecided is <m>0.0254804</m>.
</p>
<sage>
<input>
t=matrix(RR,[[0,0.4,0,0.6,0],
	[0.6,0,0.4,0,0],
	[0,0.6,0,0,0.4],
	[0,0,0,1,0],
	[0,0,0,0,1]])
t^10
</input>
<output>
[ 0.01274019840   0.0000 0.008493465600   0.8622228480   0.116543488]
[  0.0000  0.02548039680   0.0000   0.6746674176   0.299852185600000]
[ 0.01911029760   0.0000  0.01274019840   0.3933342720   0.574815232]
[  0.0000   0.0000   0.0000    1.000   0.000]
[  0.0000   0.0000   0.0000   0.0000    1.00]</output>
</sage>
<p>To summarize, </p>
<theorem>
<statement>
<p>If <m>T</m> is the transition matrix of a Markov chain, then <m>T^n_{i,j}</m> is the probability that starting in state <m>i</m>, the process will be in state <m>j</m> after <m>n</m> steps.</p>
</statement>
<proof><p>(Proof by induction)  Assume there are <m>m</m> states in a Markov chain.  Let <m>p(i,j,n)</m> be the probability that starting in state <m>i</m>, the process will be in state <m>j</m> after <m>n</m> steps.  By the definition of <m>T</m>, <m>T_{i,j} = p(i,j,1)</m> and so the basis is trivial. </p>
<p>Assume the theorem is true for some <m>n \geq 1</m>.   We observe that in the process is to end in state <m>j</m> after <m>n+1</m> steps, it could enter it from any of the <m>n</m> states and so
<me>
\begin{split}
	p(i,j,n+1)&amp;=\sum_{k=1}^m p(i,k,n)\cdot p(k,j,1)\\
		&amp;=\sum_{k=1}^m T^n_{i,k}\cdot T_{k,j}\\
		&amp;=T^{n+1}_{i,j}
\end{split}
</me>
</p>
</proof>
</theorem>
<example><title>A Bike Share Program</title> 
<p>(suggested by Kaily Burke)
Consider a citywide bike share program with three stations A, B, and C.
Bikes borrowed from one station may be returned to any station by the end
of each night. Assuming all bikes borrowed on a certain day are returned
at night, we observe from past data the following probabilities of each bikeâ€™s
subsequent state:
<ul> 
<li><p>From Station A: <m>30\%</m> of bikes return to Station A, <m>50\%</m> return to station B, <m>20\%</m> return to Station C.</p></li>
<li><p>From Station B: <m>10\%</m> return to station A, <m>60\%</m> to station B, <m>30\%</m> to station C.</p></li>
<li><p>From Station C: <m>10\%</m> return to station A, <m>10\%</m> to station B, <m>80\%</m> to station C.</p></li>
</ul>
</p>
<p>Unlike the tennis example, this process every state can be reached from every other state.  These are called transition states. The natural question to ask here is whether the distribution of bikes tend toward a steady state.  This, in fact, is true.  No matter where a bike starts, the probability that it is in any of the three stations will be largely independent of the starting state.</p>
<p>The transition matrix for this Markov chain with the ordering of states A, B, C is  
<me>
B=\left(
\begin{array}{ccc}
 0.3 &amp; 0.5 &amp; 0.2 \\
 0.1 &amp; 0.6 &amp; 0.3 \\
 0.1 &amp; 0.1 &amp; 0.8 \\
\end{array}
\right)
</me>
If we compute the seventh power of this matrix, we get the probabilities after a week. For example, the first row give the probabilities of being in states A, B and C after a week if the initial state is A.   Notice that these probabilities are not very different if the initial state is B or C.
</p>
<sage>
<input>
b=matrix(QQ,[[3/10,1/2,1/5],
            [1/10,3/5,3/10],
            [1/10,1/10,4/5]])
b^7.n()
</input>
<output>
[0.125011200 0.306755900 0.568232900]
[0.124998400 0.304168800 0.570832800]
[0.124998400 0.296356300 0.578645300]
</output>
</sage>
<p>We don't need to perform these calculations to get exact steady-state probabilities. We need only look to the one of the left eigenvectors of the transition matrix, as indicated in the following theorem, which we state here without proof. </p>
		<theorem xml:id="th-transition-evalue"><statement>Given a square matrix <m>T</m> with
		 nonnegative entries with the property that the row sums all equal to one, then one of
		its left  eigenvalues will be <m>1</m> and there exists an left eigenvector with
		 nonnegative entries.</statement>
		</theorem>
<p>We first examine the left eigenvectors of our matrix and see that 1 is first eigenvalue and it has an eigenspace of dimension one and a basis vector having positive entries.</p>
<sage>
<input>
b.eigenvectors_left()
</input>
<output>
[(1, [(1, 12/5, 23/5)], 1), (1/2, [(0, 1, -1)], 1), (1/5, [(1, -4/3, 1/3)], 1)]
</output>
</sage>
<p>We extract the first eigenvector and divide by the sum of its entries to get a probability vector.  Recall that SageMath objects are indexed starting a 0.</p>
<sage>
<input>
q=b.eigenvectors_left()[0][1][0]
p=q/sum(q)
p
</input>
<output>
(1/8, 3/10, 23/40)
</output>
</sage>
<p>We can verify that if the probabilities are in equilibrium - they stay the same if we apply them to the transition matrix.</p>
<sage>
<input>
p*b
</input>
<output>
(1/8, 3/10, 23/40)
</output>
</sage>
</example>


</subsection>