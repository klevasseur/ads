<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="s-the-diagonalization-process">
<title>The Diagonalization Process</title>
<idx>Diagonalization Process, The</idx>
<subsection><title>Eigenvalues and Eigenvectors</title>

<p>We now have the background to understand the main ideas behind the diagonalization process.</p>

<definition xml:id="def-eigenvalue-eigenvector"><title>Eigenvalue, Eigenvector</title>
<idx>Eigenvalue</idx>
<idx>Eigenvector</idx>
<statement><p>
 Let <m>A</m> be an \(n\times n\) matrix over <m>\mathbb{R}</m>.  \(\lambda\)  is an eigenvalue of \(A\) if for some nonzero column vector  \(\vec{x}\in \mathbb{R}^n\) we have \(A \vec{x} = \lambda  \vec{x}\).  \(\vec{x}\) is called an <term>eigenvector</term>
corresponding to the <term>eigenvalue</term> \(\lambda\).</p></statement></definition>
<example xml:id="ex-some-evalues"><title>Examples of eigenvalues and eigenvectors</title><p>Find the eigenvalues and corresponding eigenvectors of the matrix \(A=\left(
\begin{array}{cc}
 2 &amp; 1 \\
 2 &amp; 3 \\
\end{array}
\right)\).</p>
<p>We want to find nonzero vectors  \(\vec{x} = \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)\) and real numbers \(\lambda\)  such that
<mdn>
<mrow xml:id="equation-evalue-logic">\begin{split}
A X = \lambda  X \quad &amp;\Leftrightarrow \left(
			\begin{array}{cc}
			 2 &amp; 1 \\
			 2 &amp; 3 \\
			\end{array}
			\right)\left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right) = \lambda  \left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)\\
 	&amp; \Leftrightarrow \left(
			\begin{array}{cc}
			 2 &amp; 1 \\
			 2 &amp; 3 \\
			\end{array}
			\right)\left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right) - \lambda  \left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)=\left(
			\begin{array}{c}
			 0 \\
			 0 \\
			\end{array}
			\right)\\
	&amp; \Leftrightarrow  \left(
			\begin{array}{cc}
			 2 &amp; 1 \\
			 2 &amp; 3 \\
			\end{array}
			\right)\left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right) - \lambda  \left(
			\begin{array}{cc}
			 1 &amp; 0 \\
			 0 &amp; 1 \\
			\end{array}
			\right) \left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)=\left(
			\begin{array}{c}
			 0 \\
			 0 \\
			\end{array}
			\right)\\
	&amp; \Leftrightarrow  \left( \left(
			\begin{array}{cc}
			 2 &amp; 1 \\
			 2 &amp; 3 \\
			\end{array}
			\right) - \lambda  \left(
			\begin{array}{cc}
			 1 &amp; 0 \\
			 0 &amp; 1 \\
			\end{array}
			\right) \right)\left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)=\left(
			\begin{array}{c}
			 0 \\
			 0 \\
			\end{array}
			\right)\\
	&amp; \Leftrightarrow \left(
			\begin{array}{cc}
			 2-\lambda  &amp; 1 \\
			 2 &amp; 3-\lambda  \\
			\end{array}
			\right) \left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)=\left(
			\begin{array}{c}
			 0 \\
			 0 \\
			\end{array}
			\right)\\
\end{split}</mrow>
</mdn>
</p>

<p>The last matrix equation will have nonzero solutions if and only if



\[\det  \left(
\begin{array}{cc}
 2-\lambda  &amp; 1 \\
 2 &amp; 3-\lambda  \\
\end{array}
\right) =0\]



or  \((2 - \lambda )(3 -\lambda ) - 2 = 0\), which simplifies to \(\lambda^2 - 5\lambda  + 4 = 0\).   Therefore, the solutions to this quadratic equation, \(\lambda_1 = 1\) and \(\lambda_2 = 4\), are the eigenvalues of <m>A</m>. We now have to find eigenvectors associated with each eigenvalue.</p>

<p>Case 1. For \(\lambda_1= 1\),  <xref ref="equation-evalue-logic" text="type-global" />  becomes:

\[\left(
\begin{array}{cc}
 2-1 &amp; 1 \\
 2 &amp; 3-1 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right) \\
\\
\left(
\begin{array}{cc}
 1 &amp; 1 \\
 2 &amp; 2 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right)\textrm{    } \]
which reduces to the single equation, \(x_1+ x_2= 0\).  From this, \(x_1= -x_2\). This means the solution set of this equation is (in column notation)
\[E_1 = \left\{ \left.\left(
\begin{array}{c}
 -c \\
 c \\
\end{array}
\right) \right| c\in  \mathbb{R}\right\}\]



So any column vector of the form \(\left(
\begin{array}{c}
 -c \\
 c \\
\end{array}
\right)\) where <m>c</m> is any nonzero real number is an eigenvector associated with  \(\lambda_1=1\).   The reader should verify that,
for example, 

\[\left(
\begin{array}{cc}
 2 &amp; 1 \\
 2 &amp; 3 \\
\end{array}
\right)\left(
\begin{array}{c}
 \frac{2}{3} \\
 -\frac{2}{3} \\
\end{array}
\right) = 1 \left(
\begin{array}{c}
 \frac{2}{3} \\
 -\frac{2}{3} \\
\end{array}
\right)\]
so that \(\left(
\begin{array}{c}
 \frac{2}{3} \\
 -\frac{2}{3} \\
\end{array}
\right)\) is an eigenvector associated with eigenvalue 1.</p>

<p>Case 2.  For \(\lambda_2=4\) <xref ref="equation-evalue-logic" text="type-global" /> becomes:
\[\left(
\begin{array}{cc}
 2-4 &amp; 1 \\
 2 &amp; 3-4 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right) \\
\\
\left(
\begin{array}{cc}
 -2 &amp; 1 \\
 2 &amp; -1 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right) \]
which reduces to the single equation \(-2x_1+x_2=0\), so that \(x_2= 2x_1\). The solution set of the equation is



\[E_2=\left\{\left.\left(
\begin{array}{c}
 c \\
 2c \\
\end{array}
\right)\right| c\in  \mathbb{R}\right\}\]
Therefore, all eigenvectors of <m>A</m> associated with the eigenvalue \(\lambda_2 = 4\) are of the form \(\left(
\begin{array}{c}
 c \\
 2c  \\
\end{array}
\right)\), where <m>c</m> can be any nonzero number.
</p></example>

<p>The following theorems summarize the most important aspects of the previous example.</p>

<theorem xml:id="theorem-evalue-det-theorem"><title>Characterization of Eigenvalues of a Square Matrix</title>
<statement><p>Let <m>A</m> be any \(n\times n\) matrix over \(\mathbb{R}\). Then \(\lambda \in  \mathbb{R}\) is an eigenvalue of \(A\) if and only if \(\det (A - \lambda  I) = 0\).</p></statement>
</theorem> 

<p>The equation \(\det (A - \lambda  I) = 0\) is called the <term>characteristic equation</term>, and the left side of this equation is called the <term>
characteristic polynomial</term> of <m>A</m>.</p>

<theorem xml:id="theorem-evector-independence"><title>Linear Independence of Eigenvectors</title>
<statement><p>Nonzero eigenvectors corresponding to distinct eigenvalues are linearly independent.</p></statement></theorem>

<p>The solution space of \((A-\lambda  I)\vec{x}=\vec{0}\) is called the eigenspace of \(A\) corresponding to \(\lambda\). This terminology is justified by Exercise 2 of this section.</p>
</subsection>
<subsection><title>Diagonalization</title>

<p>We now consider the main aim of this section. Given an \(n\times n\) (square) matrix <m>A</m>, we would like to transform <m>A</m> into a diagonal matrix <m>D</m>, perform our tasks with the simpler matrix <m>D</m>, and then describe the results in terms of the given matrix \(A\).</p>
<definition xml:id="def-diagonalizable-matrix">
<title>Diagonalizable Matrix.</title>
<idx>Diagonalizable Matrix</idx>
<statement><p>An \(n\times n\) matrix <m>A</m> is called diagonalizable if there exists an invertible \(n\times n\)
matrix <m>P</m> such that \(P^{-1} A P\) is a diagonal matrix <m>D</m>. The matrix <m>P</m> is said to diagonalize the matrix <m>A</m>.</p></statement></definition>
<example xml:id="ex-diagonalization-of-matrix">
<title>Diagonalization of a Matrix</title>
<p>We will now diagonalize the matrix <m>A</m> of <xref ref="ex-some-evalues" text="type-global" />.  We form the matrix <m>P</m> as follows: Let \(P^{(1)}\)
be the first column of <m>P</m>.  Choose for \(P^{(1)}\) any eigenvector from \(E_1\). We may as well choose a simple vector in \(E_1\) so \(P^{(1)}=\left(
\begin{array}{c}
 1 \\
 -1 \\
\end{array}
\right)\) is our candidate.</p>
<p>Similarly, let \(P^{(2)}\)  be the second column of <m>P</m>, and choose for \(P^{(2)}\) any eigenvector from \(E_2\). The vector \(P^{(2)}=\left(
\begin{array}{c}
 1 \\
 2 \\
\end{array}
\right)\)  is a  reasonable choice, thus 
\[P= \left(
\begin{array}{cc}
 1 &amp; 1 \\
 -1 &amp; 2 \\
\end{array}
\right)    \textrm{ and }	  P^{-1}= \frac{1}{3}\left(
\begin{array}{cc}
 2 &amp; -1 \\
 1 &amp; 1 \\
\end{array}
\right)=\left(
\begin{array}{cc}
 \frac{2}{3} &amp; -\frac{1}{3} \\
 \frac{1}{3} &amp; \frac{1}{3} \\
\end{array}
\right)\]
so that   
\[P^{-1} A P = \frac{1}{3}\left(
\begin{array}{cc}
 2 &amp; -1 \\
 1 &amp; 1 \\
\end{array}
\right)\left(
\begin{array}{cc}
 2 &amp; 1 \\
 2 &amp; 3 \\
\end{array}
\right)\left(
\begin{array}{cc}
 1 &amp; 1 \\
 -1 &amp; 2 \\
\end{array}
\right) = \left(
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\]



Notice that the elements on the main diagonal of <m>D</m> are the eigenvalues of <m>A</m>, where \(D_{i i}\) is the eigenvalue corresponding
to the eigenvector \(P^{(i)}\) .
</p></example>
<note><p><ol label="(1)">
<li><p> The first step in the diagonalization process is the determination of the eigenvalues. The ordering of the eigenvalues is purely arbitrary.
If we designate \(\lambda_1 = 4\)  and \(\lambda_2=1\), the columns of \(P\) would be interchanged and <m>D</m> would be \(\left(
\begin{array}{cc}
 4 &amp; 0 \\
 0 &amp; 1 \\
\end{array}
\right)\) (see Exercise 3b of this section).  Nonetheless, the final outcome of the application to which we are applying the diagonalization process
would be the same.</p></li>
<li><p> If <m>A</m> is an \(n\times n\) matrix with distinct eigenvalues, then <m>P</m> is also an \(n\times n\) matrix whose columns \(P^{(1)}\),
\(P^{(2)}, \ldots\), \(P^{(n)}\) are <m>n</m> linearly independent vectors.</p></li>
</ol>
</p>
</note>

<example xml:id="ex-another-diagonalization"><title>Diagonalization of a 3 by 3 matrix</title><p> Diagonalize the matrix
\[A= \left(
\begin{array}{ccc}
 1 &amp; 12 &amp; -18 \\
 0 &amp; -11 &amp; 18 \\
 0 &amp; -6 &amp; 10 \\
\end{array}
\right)\].</p>
<p>First, we find the eigenvalues of \(A\). 
<me>
\begin{split}
\det (A-\lambda  I) &amp;=\det \left(
			\begin{array}{ccc}
			 1-\lambda  &amp; 12 &amp; -18 \\
			 0 &amp; -\lambda -11 &amp; 18 \\
			 0 &amp; -6 &amp; 10-\lambda  \\
			\end{array}
			\right)\\
	&amp;=(1-\lambda ) \det  \left(
			\begin{array}{cc}
			 -\lambda -11 &amp; 18 \\
			 -6 &amp; 10-\lambda  \\
			\end{array}
			\right)\\
	&amp;=(1-\lambda ) ((-\lambda -11)(10-\lambda )+108) = (1-\lambda ) \left(\lambda ^2+\lambda -2\right)
\end{split}
</me>
Hence, the equation \(\det (A-\lambda  I)\) becomes
\[(1-\lambda ) \left(\lambda ^2+\lambda -2\right) =- (\lambda -1)^2(\lambda +2)\]
Therefore, our eigenvalues for <m>A</m> are \(\lambda_1= -2\) and \(\lambda_2=1\). We note that we do not have three distinct eigenvalues, but
we proceed as in the previous example.</p>
<p>Case 1.  For \(\lambda_1= -2\) the equation \((A-\lambda  I)\vec{x}= \vec{0}\)  becomes
\[\left(
\begin{array}{ccc}
 3 &amp; 12 &amp; -18 \\
 0 &amp; -9 &amp; 18 \\
 0 &amp; -6 &amp; 12 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\]
We can row reduce the matrix of coefficients to \(\left(
\begin{array}{ccc}
 1 &amp; 0 &amp; 2 \\
 0 &amp; 1 &amp; -2 \\
 0 &amp; 0 &amp; 0 \\
\end{array}
\right)\).</p>
<p>The matrix equation is then equivalent to the equations \(x_1 = -2x_3 \textrm{ and } x_2= 2x_3\).  Therefore, the solution set, or eigenspace, corresponding to \(\lambda_1=-2\) consists of vectors of the form 
\[\left(
\begin{array}{c}
 -2x_3 \\
 2x_3 \\
 x_3 \\
\end{array}
\right)= x_3\left(
\begin{array}{c}
 -2 \\
 2 \\
 1 \\
\end{array}
\right)\]
</p>
<p>Therefore \(\left(
\begin{array}{c}
 -2 \\
 2 \\
 1 \\
\end{array}
\right)\) is an eigenvector corresponding to the eigenvalue \(\lambda_1=-2\), and can be used for our first column of \(P\):
\[P= \left(
\begin{array}{ccc}
 -2 &amp; ? &amp; ? \\
 2 &amp; ? &amp; ? \\
 1 &amp; ? &amp; ? \\
\end{array}
\right)\]
</p>
<p>Before we continue we make the observation: \(E_1\) is a subspace of \(\mathbb{R}^3\) with basis \(\left\{P^{(1)}\right\}\) and  \(\dim  E_1 =
1\).</p>
<p>Case 2. If \(\lambda_2= 1\), then the equation \((A-\lambda  I)\vec{x}= \vec{0}\) becomes
 \[\left(
\begin{array}{ccc}
 0 &amp; 12 &amp; -18 \\
 0 &amp; -12 &amp; 18 \\
 0 &amp; -6 &amp; 9 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\]
</p>

<p>Without the aid of any computer technology, it should be clear that all three equations that correspond to this matrix equation are equivalent to
 \(2 x_2-3x_3= 0\), or \(x_2= \frac{3}{2}x_3\).    Notice that \(x_1\) can take on any value, so any vector of the form
\[\left(
\begin{array}{c}
 x_1 \\
 \frac{3}{2}x_3 \\
 x_3 \\
\end{array}
\right)=x_1\left(
\begin{array}{c}
 1 \\
 0 \\
 0 \\
\end{array}
\right)+x_3\left(
\begin{array}{c}
 0 \\
 \frac{3}{2} \\
 1 \\
\end{array}
\right)\]
will solve the matrix equation.</p>

<p>We note that the solution set contains two independent variables, \(x_1\) and \(x_3\). Further, note that we cannot express the eigenspace \(E_2\) as a linear combination of a single vector as in Case 1.   However, it can be written as 
\[E_2= \left\{x_1\left(
\begin{array}{c}
 1 \\
 0 \\
 0 \\
\end{array}
\right)+x_3\left(
\begin{array}{c}
 0 \\
 \frac{3}{2} \\
 1 \\
\end{array}
\right) \mid x_1,x_3\in  \mathbb{R}\right\}.\]
</p>

<p>We can replace any vector in a basis is with a nonzero multiple of that vector.  Simply for aesthetic reasons, we will multiply the second vector
that generates \(E_2\) by 2.  Therefore, the eigenspace \(E_2\) is a subspace of \(\mathbb{R}^3\) with basis \(\left\{\left(
\begin{array}{c}
 1 \\
 0 \\
 0 \\
\end{array}
\right),\left(
\begin{array}{c}
 0 \\
 3 \\
 2 \\
\end{array}
\right)\right\}\) and so  \(\dim  E_2 = 2\).</p>


<p>What this means with respect to the diagonalization process is that \(\lambda_2= 1\)  gives us both Column 2 and Column 3 the diagonalizing matrix.
  The order is not important so we have 


\[P= \left(
\begin{array}{ccc}
 -2 &amp; 1 &amp; 0 \\
 2 &amp; 0 &amp; 3 \\
 1 &amp; 0 &amp; 2 \\
\end{array}
\right)\]</p>



<p>The reader can verify (see Exercise 5 of this section) that



 \(P^{-1}= \left(
\begin{array}{ccc}
 0 &amp; 2 &amp; -3 \\
 1 &amp; 4 &amp; -6 \\
 0 &amp; -1 &amp; 2 \\
\end{array}
\right)\)   and   \(P^{-1}A P = \left(
\begin{array}{ccc}
 -2 &amp; 0 &amp; 0 \\
 0 &amp; 1 &amp; 0 \\
 0 &amp; 0 &amp; 1 \\
\end{array}
\right)\)
</p></example>


<p>In doing <xref ref="ex-another-diagonalization" text="type-global" />, the given \(3\times 3\) matrix <m>A</m> produced only two, not three, distinct eigenvalues, yet we were still able to
diagonalize <m>A</m>. The reason we were able to do so was because we were able to find three linearly independent eigenvectors. Again, the main
idea is to produce a matrix \(P\) that does the diagonalizing. If <m>A</m> is an \(n \times  n\) matrix, <m>P</m> will be an \(n\times
n\) matrix, and its <m>n</m> columns must be linearly independent eigenvectors. The main question in the study of diagonalizability is <q>When
can it be done?</q> This is summarized in the following theorem.</p>

<theorem xml:id="theorem-diagon-condition"><title>A condition for diagonalizability</title><statement><p> Let \(A\) be an \(n \times  n\) matrix.   Then \(A\) is diagonalizable if and only if \(A\) has \(n\) linearly independent eigenvectors.</p></statement>
<proof><p>Outline of a proof: (\(\Longleftarrow\)) Assume that <m>A</m> has linearly independent eigenvectors, \(P^{(1)}\), \(P^{(2)}\), $\ldots $, \(P^{(n)}\),
with corresponding eigenvalues \(\lambda_1\), \(\lambda_2, \ldots\) , \(\lambda _n\).   We want to prove that <m>A</m> is diagonalizable.
Column <m>i</m> of the \(n \times n\) matrix \(A P\) is  \(A P^{(i)}\) (see Exercise 7 of this section). Then, since the \(P^{(i)}\) is an eigenvector
of <m>A</m>  associated  with the eigenvalue \(\lambda _i\) we have \(A P^{(i)}= \lambda _iP^{(i)}\) for\(i = 1, 2, \dots , n\). But this
means that \(A P = P D\), where \(D\) is the diagonal matrix with diagonal entries  \(\lambda_1\), \(\lambda_2,\ldots \), \(\lambda_n\).   If we multiply both sides of the equation by \(P^{-1}\) we get the desired \(P^{-1}A P = D\).</p>



<p>(\(\Longrightarrow \)) The proof in this direction involves a concept that is not covered in this text (rank of a matrix); so we refer the interested
reader to virtually any linear algebra text for a proof.  
</p></proof></theorem>

<p>We now give an example of a matrix that is not diagonalizable.</p>

<example xml:id="ex-not-diagonalizable">
<title>A Matrix that is Not Diagonalizable</title>
<p> Let us attempt to diagonalize the matrix \(A = \left(
\begin{array}{ccc}
 1 &amp; 0 &amp; 0 \\
 0 &amp; 2 &amp; 1 \\
 1 &amp; -1 &amp; 4 \\
\end{array}
\right)\)</p>


<p>First, we determine the eigenvalues.

<me>
\begin{split}
\det (A-\lambda  I) &amp;= \det  \left(
			\begin{array}{ccc}
			 1-\lambda  &amp; 0 &amp; 0 \\
			 0 &amp; 2-\lambda  &amp; 1 \\
			 1 &amp; -1 &amp; 4-\lambda  \\
			\end{array}
			\right)\\
		&amp;= (1-\lambda) \det \left(
			\begin{array}{cc}
			 2-\lambda  &amp; 1 \\
			 -1 &amp; 4-\lambda  \\
			\end{array}
			\right)\\
	&amp; = (1-\lambda )((2-\lambda )(4-\lambda )+1)\\
	&amp; = (1-\lambda )\left(\lambda ^2-6\lambda  +9\right)\\
	&amp; = (1-\lambda)(\lambda -3)^2 
\end{split}
</me>

Therefore there are two eigenvalues, \(\lambda_1= 1\) and \(\lambda_2=3\).  Since \(\lambda_1\) is an eigenvalue of degree one, it will have an eigenspace
of dimension 1.  Since \(\lambda_2\) is a double root of the characteristic equation, the dimension of its eigenspace must be 2 in order to be able to diagonalize.</p>



<p>Case 1. For \(\lambda_1= 1\),  the equation \((A-\lambda  I)\vec{x} = \vec{0}\) becomes







\[\left(
\begin{array}{ccc}
 0 &amp; 0 &amp; 0 \\
 0 &amp; 1 &amp; 1 \\
 1 &amp; -1 &amp; 3 \\
\end{array}
\right)\left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\]
</p>

<p>Row reduction of this system reveals one free variable and eigenspace 


\[\left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)=\left(
\begin{array}{c}
 -4x_3 \\
 -x_3 \\
 x_3 \\
\end{array}
\right)= x_3\left(
\begin{array}{c}
 -4 \\
 -1 \\
 1 \\
\end{array}
\right)\]



Hence,  \(\left\{\left(
\begin{array}{c}
 -4 \\
 -1 \\
 1 \\
\end{array}
\right)\right\}\) is a basis for the eigenspace of \(\lambda_1= 1\).  </p>



<p>Case 2.  For \(\lambda_2= 3\),  the equation \((A-\lambda  I)\vec{x} = \vec{0}\) becomes



  \[\left(
\begin{array}{ccc}
 -2 &amp; 0 &amp; 0 \\
 0 &amp; -1 &amp; 1 \\
 1 &amp; -1 &amp; 1 \\
\end{array}
\right)\left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\]

</p>


<p>Once again there is only one free variable in the row reduction and so the dimension of the eigenspace will be one:



 \[\left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 x_3 \\
 x_3 \\
\end{array}
\right)= x_3\left(
\begin{array}{c}
 0 \\
 1 \\
 1 \\
\end{array}
\right)\]



Hence,  \(\left\{\left(
\begin{array}{c}
 0 \\
 1 \\
 1 \\
\end{array}
\right)\right\}\) is a basis for the eigenspace of \(\lambda_2= 3\).  This means that \(\lambda_2= 3\) produces only one column for \(P\). Since we began with only two eigenvalues, we had hoped that \(\lambda_2= 3\) would produce a vector space of dimension two, or, in matrix terms, two
linearly independent columns for <m>P</m>. Since <m>A</m> does not have three linearly independent eigenvectors  <m>A</m> cannot be diagonalized.</p></example>
</subsection>



<subsection xml:id="sage-note-diag"><title>SageMath Note - Diagonalization</title>
<idx><h>SageMath Note</h><h>Matrix Diagonalization</h></idx>
<p>We demonstrate how diagonalization can be done in Sage. We start by defining the matrix to be diagonalized, and also declare \(D\) and \(P\) to be variables.</p>

<sage>
<input>
var (' D, P')
A = Matrix (QQ, [[4, 1, 0], [1, 5, 1], [0, 1, 4]]);A
</input>
<output>
(D, P)
[4 1 0]
[1 5 1]
[0 1 4]
</output>
</sage>

<p>We have been working with <q>right eigenvectors</q> since the \(\vec{x}\) in \(A \vec{x} = \lambda  \vec{x}\) is a column vector. It's not so common but still desirable in some situations to consider <q>left eigenvectors,</q> so SageMath allows either one. The <c>right_eigenmatrix</c> method returns a pair of matrices.  The diagonal matrix, \(D\), with eigenvalues and the diagonalizing matrix, \(P\), which is made up of columns that are eigenvectors corresponding to the eigenvectors of \(D\).</p>

<sage>
<input>
(D,P)=A.right_eigenmatrix();(D,P)
</input>
<output>
([6 0 0]
[0 4 0]
[0 0 3], 
[ 1  1  1]
[ 2  0 -1]
[ 1 -1  1])
</output>
</sage>


<p>We should note here that \(P\) is not unique because even if an eigenspace has dimension one, any nonzero vector in that space will serve as an eigenvector.
  For that reason, the \(P\) generated by Sage isn't necessarily the same as the one computed by any other computer algebra system such as Mathematica.  Here we verify
the result for our Sage calculation.  Recall that an asterisk is used for matrix multiplication in Sage.</p>

<sage>
<input>
P.inverse()*A*P
</input>
<output>
[6 0 0]
[0 4 0]
[0 0 3]
</output>
</sage>


<p>Here is a second matrix to diagonalize.</p>
<sage>
<input>
A2=Matrix(QQ,[[8,1,0],[1,5,1],[0,1,7]]);A2
</input>
<output>
[8 1 0]
[1 5 1]
[0 1 7]
</output>
</sage>


<p>Here we've already specified that the underlying system is the rational numbers.  Since the eigenvalues are not rational, Sage will revert to approximate
number by default. We'll just pull out the matrix of eigenvectors this time and display rounded entries.</p>
<sage>
<input>
P=A2.right_eigenmatrix()[1]
P.numerical_approx(digits=3)
print '------------------'
D=(P.inverse()*A2*P);D.numerical_approx(digits=3)
</input>
<output>
[  1.00   1.00   1.00]
[ -3.65 -0.726  0.377]
[  1.38  -2.65  0.274]
------------------
[ 4.35 0.000 0.000]
[0.000  7.27 0.000]
[0.000 0.000  8.38]
</output>
</sage>

<p>Finally, we examine how Sage reacts to the matrix from <xref ref="ex-not-diagonalizable" text="type-global" /> that couldn't be diagonalized.   Notice that the last column is a zero column, indicating the absence of one needed eigenvector.</p>
<sage>
<input>
A3=Matrix(QQ,[[1, 0, 0],[0,2,1],[1,-1,4]])
(D,P)=A3.right_eigenmatrix();(D,P)
</input>
<output>
([1 0 0]
[0 3 0]
[0 0 3], 
[   1    0    0]
[ 1/4    1    0]
[-1/4    1    0])
</output>
</sage>
</subsection>
<exercises xml:id="exercises-12-4">
<title>Exercises for Section 12.4</title>

<exercise number="1"><statement><p><ol label="(a)"><li><p>List three different eigenvectors of \(A=\left(
\begin{array}{cc}
 2 &amp; 1 \\
 2 &amp; 3 \\
\end{array}
\right)\), the matrix of <xref ref="ex-some-evalues" text="type-global" />, associated with each of the two eigenvalues 1 and 4.   Verify your results.</p></li>
<li><p> Choose one of the three eigenvectors corresponding to 1 and one of the three eigenvectors corresponding to 4, and show that the two chosen
vectors are linearly independent.</p></li>
</ol></p>
</statement>
<answer><p>
<ol label="(a)">
<li><p> Any nonzero multiple of \(\left(
\begin{array}{c}
 1 \\
 -1 \\
\end{array}
\right)\) is an eigenvector associated with \(\lambda =1\).</p></li>
<li><p>  Any nonzero multiple of \(\left(
\begin{array}{c}
 1 \\
 2 \\
\end{array}
\right)\) is an eigenvector associated with \(\lambda =4\).</p></li>
<li><p>  Let \(x_1=\left(
\begin{array}{c}
 a \\
 -a \\
\end{array}
\right)\) and \(x_2=\left(
\begin{array}{c}
 b \\
 2b \\
\end{array}
\right)\) .  You can verify that  \(c_1x_1+ c_2x_2=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right)\)  if and only if \(c_1= c_2= 0.\)  Therefore, \(\left\{x_1,x_2\right\}\) is linearly independent.</p></li>
</ol>
</p></answer></exercise>
<exercise number="2"><statement><p><ol label="(a)"><li><p>Verify that \(E_1\) and \(E_2\) in Example 12.4.1 are vector spaces over <m>\mathbb{R}</m>.  Since they are also subsets of \(\mathbb{R}^2\),
they are called subvector-spaces, or subspaces for short, of \(\mathbb{R}^2\). Since these are subspaces consisting of eigenvectors, they are called
eigenspaces. </p></li>
<li><p> Use the definition of dimension in the previous section to find \(\dim E_1\) and \(\dim  E_2\) . Note that \(\dim E_1 + dim E_2 = \dim \mathbb{R}^2\). This is not a coincidence.</p></li>
</ol></p>
</statement></exercise>
<exercise number="3"><statement><p><ol label="(a)"><li><p>Verify that \(P^{-1} A P\) is indeed equal to \(\left(
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\),  as indicated in <xref ref="ex-diagonalization-of-matrix" text="type-global" /></p></li>
<li><p> Choose \(P^{(1)}=\left(
\begin{array}{c}
 1 \\
 2 \\
\end{array}
\right)\) and \(P^{(2)}=\left(
\begin{array}{c}
 1 \\
 -1 \\
\end{array}
\right)\) and verify that the new value of \(P\) satisfies \(P^{-1} A P=\left(
\begin{array}{cc}
 4 &amp; 0 \\
 0 &amp; 1 \\
\end{array}
\right)\)</p></li>
<li><p>  Take any two linearly independent eigenvectors of the matrix <m>A</m> of <xref ref="ex-diagonalization-of-matrix" text="type-global" /> and verify that \(P^{-1} A P\) is a diagonal matrix.</p></li>
</ol></p>
</statement>
<answer>
<p>Part c: You should obtain \(\left(
\begin{array}{cc}
 4 &amp; 0 \\
 0 &amp; 1 \\
\end{array}
\right)\) or \(\left(
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\), depending on how you order the eigenvalues. 
</p></answer></exercise>
<exercise number="4"><statement><p><ol label="(a)"><li><p>Let A be the matrix in Example 12.4.3 and \(P=\left(
\begin{array}{ccc}
 0 &amp; 1 &amp; 0 \\
 1 &amp; 0 &amp; 1 \\
 1 &amp; 0 &amp; 2 \\
\end{array}
\right)\).  Without doing any actual matrix multiplications, determine the value of \(P^{-1} A P\)</p></li>
<li><p>  If you choose the columns of \(P\) in the reverse order, what is \(P^{-1} A P\)?</p></li>
</ol></p>
</statement></exercise>
<exercise number="5">
<statement> <p> Diagonalize the following, if possible:
<ol label="(a)" cols="3">
<li><p>    \(\left(
\begin{array}{cc}
 1 &amp; 2 \\
 3 &amp; 2 \\
\end{array}
\right)\)</p></li>
<li><p> \(\left(
\begin{array}{cc}
 -2 &amp; 1 \\
 -7 &amp; 6 \\
\end{array}
\right)\) </p></li>
<li><p>  \(\left(
\begin{array}{cc}
 3 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\)
</p></li>
<li><p>    \(\left(
\begin{array}{ccc}
 1 &amp; -1 &amp; 4 \\
 3 &amp; 2 &amp; -1 \\
 2 &amp; 1 &amp; -1 \\
\end{array}
\right)\)</p></li>
<li><p>  \(\left(
\begin{array}{ccc}
 6 &amp; 0 &amp; 0 \\
 0 &amp; 7 &amp; -4 \\
 9 &amp; 1 &amp; 3 \\
\end{array}
\right)\)</p></li>
<li><p>  \(\left(
\begin{array}{ccc}
 1 &amp; -1 &amp; 0 \\
 -1 &amp; 2 &amp; -1 \\
 0 &amp; -1 &amp; 1 \\
\end{array}
\right)\)</p></li>
</ol></p>
</statement>
<answer><p>
<ol label="(a)" cols="2">
<li><p>  If  \(P=\left(
\begin{array}{cc}
 2 &amp; 1 \\
 3 &amp; -1 \\
\end{array}
\right)\), then \(P^{-1}A P=\left(
\begin{array}{cc}
 4 &amp; 0 \\
 0 &amp; -1 \\
\end{array}
\right)\).</p></li>
<li><p> If  \(P=\left(
\begin{array}{cc}
 1 &amp; 1 \\
 7 &amp; 1 \\
\end{array}
\right)\), then \(P^{-1}A P=\left(
\begin{array}{cc}
 5 &amp; 0 \\
 0 &amp; -1 \\
\end{array}
\right)\).</p></li>
<li><p> If  \(P=\left(
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; 1 \\
\end{array}
\right)\), then \(P^{-1}A P=\left(
\begin{array}{cc}
 3 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\).</p></li>
<li><p> If  \(P=\left(
\begin{array}{ccc}
 1 &amp; -1 &amp; 1 \\
 -1 &amp; 4 &amp; 2 \\
 -1 &amp; 1 &amp; 1 \\
\end{array}
\right)\), then \(P^{-1}A P=\left(
\begin{array}{ccc}
 -2 &amp; 0 &amp; 0 \\
 0 &amp; 1 &amp; 0 \\
 0 &amp; 0 &amp; 0 \\
\end{array}
\right)\).</p></li>
<li><p> <m>A</m> is not diagonalizable. Five is a double root of the characteristic equation, but has an eigenspace with dimension only 1.</p></li>
<li><p>  If  \(P=\left(
\begin{array}{ccc}
 1 &amp; 1 &amp; 1 \\
 -2 &amp; 0 &amp; 1 \\
 1 &amp; -1 &amp; 1 \\
\end{array}
\right)\), then \(P^{-1}A P=\left(
\begin{array}{ccc}
 3 &amp; 0 &amp; 0 \\
 0 &amp; 1 &amp; 0 \\
 0 &amp; 0 &amp; 0 \\
\end{array}
\right)\).</p></li>
</ol>
</p></answer>
</exercise>
<exercise number="6">
<statement><p>Diagonalize the following, if possible:
<ol label="(a)" cols="3">
<li><p>    \(\left(
\begin{array}{cc}
 0 &amp; 1 \\
 1 &amp; 1 \\
\end{array}
\right)\)</p></li>
<li><p>  \(\left(
\begin{array}{cc}
 2 &amp; 1 \\
 4 &amp; 2 \\
\end{array}
\right)\)   </p></li>
<li><p>   \(\left(
\begin{array}{cc}
 2 &amp; -1 \\
 1 &amp; 0 \\
\end{array}
\right)\)</p></li>
<li><p>  \(\left(
\begin{array}{ccc}
 1 &amp; 3 &amp; 6 \\
 -3 &amp; -5 &amp; -6 \\
 3 &amp; 3 &amp; 6 \\
\end{array}
\right)\) </p></li>
<li><p>   \(\left(
\begin{array}{ccc}
 1 &amp; 1 &amp; 0 \\
 1 &amp; 0 &amp; 1 \\
 0 &amp; 1 &amp; 1 \\
\end{array}
\right)\)</p></li>
<li><p>   \(\left(
\begin{array}{ccc}
 2 &amp; -1 &amp; 0 \\
 -1 &amp; 2 &amp; -1 \\
 0 &amp; -1 &amp; 2 \\
\end{array}
\right)\)</p></li>
</ol></p>
</statement>
</exercise>
<exercise number="7"><statement> <p> Let <m>A</m> and <m>P</m> be as in <xref ref="ex-another-diagonalization" text="type-global" />. Show that the columns of the matrix \(A P\) can be found by computing \(A P^{(1)}\),
\(A P^{(2)},\ldots,\) \(A P^{(n)}\). </p>
</statement>
<answer><p>This is a direct application of the definition of matrix multiplication. Let \(A_{(i)}\) be the \(i^{\textrm{th}}\) row of <m>A</m>,
and let \(P^{(j)}\) be the \(j^{\textrm{th}}\) column of  <m>P</m>.  Then the \(j^{\textrm{th}}\) column of the product \(A P\) is
\[\left(
\begin{array}{c}
 A_{(1)}P^{(j)} \\
 A_{(2)}P^{(j)} \\
 \vdots  \\
 A_{(n)}P^{(j)} \\
\end{array}
\right)\]
</p>
<p>Hence, \((AP)^{(j)}= A\left(P^{(j)}\right)\)  for \(j =1,2,\ldots , n\). Thus, each column of \(A P\) depends on <m>A</m> and the \(j^{\textrm{ th}}\)
column of <m>P</m>.</p></answer>
</exercise>
<exercise number="8"><statement><p>  Prove that if \(P\) is an \(n\times n\) matrix and \(D\) is a diagonal matrix with diagonal entries \(d_1\), \(d_2,\ldots,\) \(d_n\), then
\(P D\) is the matrix obtained from \(P\), by multiplying column <m>i</m> of <m>P</m> by \(d_i\), \(i = 1, 2, \ldots, n\).</p>
</statement></exercise>
</exercises>
</section>